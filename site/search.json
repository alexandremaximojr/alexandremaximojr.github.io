[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Previsão de Séries Temporais - Aplicações Econométricas (Machine learning)\nAutor: Alexandre Maximo de Souza Júnior\nData: 05/2023"
  },
  {
    "objectID": "favorita_sales.html",
    "href": "favorita_sales.html",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "A previsão de vendas desempenha um papel crucial na gestão de negócios, permitindo que as empresas planejem adequadamente suas operações e tomem decisões informadas. A capacidade de prever com precisão a demanda futura é especialmente importante para varejistas com lojas físicas, como supermercados, que precisam equilibrar cuidadosamente seus estoques para evitar desperdício de alimentos perecíveis e atender às expectativas dos clientes. A utilização de técnicas de previsão baseadas em aprendizado de máquina pode ajudar a aprimorar essa tarefa, oferecendo maior acurácia e eficiência.\nNeste trablaho, selecionamos a série temporal de vendas de produtos da Corporación Favorita, uma grande varejista de supermercados com sede no Equador. Faremos uma aplicação empírica prever a demanda da família de produtos ‘bebidas’ da rede de supermercados corporación favorita, mais especificamente de sua unidade 44.\n\n\n\n\nPrimeiramente, vamos realizar as importações das bibliotecas que serão utilizadas:\n\n# Importação das bibliotecas\n\nimport keras\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.graph_objects as go\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom statsmodels.tsa.stattools import adfuller\nfrom datetime import timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pmdarima.arima import auto_arima\nfrom pmdarima import arima\n\n\n\n\n# Importação dos datasets\n\noil = pd.read_csv('oil.csv')\nvendas = pd.read_csv('train.csv')\nlojas = pd.read_csv('stores.csv')\ntransacoes = pd.read_csv('transactions.csv')\nferiados = pd.read_csv('holidays_events.csv')\n\n\n\n\n\n# Definição das características da previsão\n\nnumero_loja = 44 # Número da loja que será prevista\nn = 30 # Número de dias que será previsto no modelo\nfamilia = 'BEVERAGES' # Família de produtos que será prevista\n\n\n# Transformações nos dados\n\n# Unir os datasets\nvendas = pd.merge(vendas, oil, how='left', on='date')\nvendas = pd.merge(vendas, lojas, how='left', on='store_nbr')\n\n# Filtrar apenas a loja que será projetada\nvendas = vendas[vendas['store_nbr'] == numero_loja]\nvendas = vendas[vendas['family'] == familia]\nvendas = vendas.reset_index(drop=True)\n\n# Transformações nas informações do preço do petróleo\nvendas = vendas.rename(columns={\"dcoilwtico\":\"oil_price\"}) # Renomear a coluna para oil_price\nvendas['oil_price'] = vendas['oil_price'].fillna(vendas['oil_price'].mode()[0]) # Valores NA preenchidos com a moda\n\ndel feriados, oil, lojas # Deletar bases não utilizadas após uniões\n\n# Converter  as datas para formato correto\nvendas['date'] = pd.to_datetime(vendas['date']) # Converter a coluna 'date' para o tipo datetime\nvendas['cluster'] = vendas['cluster'].astype('category')\n#dummies = pd.get_dummies(vendas['family'])\n#vendas = pd.concat([vendas, dummies], axis=1)\n\n# Removendo colunas que não serão utilizadas\nvendas = vendas.drop('id', axis=1)\nvendas = vendas.drop('onpromotion', axis=1)\nvendas = vendas.drop('city', axis=1)\nvendas = vendas.drop('state', axis=1)\nvendas = vendas.drop('store_nbr', axis=1)\nvendas = vendas.drop('type', axis=1)\n\n# Visualização do dataframe vendas para verificação\n#display(vendas)\nvendas.info() #Verificar os tipos das variaveis na base\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1684 entries, 0 to 1683\nData columns (total 5 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       1684 non-null   datetime64[ns]\n 1   family     1684 non-null   object        \n 2   sales      1684 non-null   float64       \n 3   oil_price  1684 non-null   float64       \n 4   cluster    1684 non-null   category      \ndtypes: category(1), datetime64[ns](1), float64(2), object(1)\nmemory usage: 54.5+ KB\n\n\n\n\n\n\n# Divisão entre amostra de treino e teste\ndata_corte = vendas['date'].max() - timedelta(days=n) #Calcula a data de corte entre treino e teste\nvendas_treino = vendas.loc[vendas['date'] &lt;= data_corte]\nvendas_teste = vendas.loc[vendas['date'] &gt; data_corte]\n\n# Modificar estrutura de data para colunas\nvendas['Ano'] = vendas['date'].dt.year\nvendas['Mês'] = vendas['date'].dt.month\nvendas['Dia'] = vendas['date'].dt.day\n#vendas = vendas.drop('date', axis=1)\n\nvendas['Ano'] = vendas['Ano'].astype('category')\nvendas['Mês'] = vendas['Mês'].astype('category')\nvendas['Dia'] = vendas['Dia'].astype('category')\n\n#display(vendas_treino)\n#display(vendas_teste)\n\n\n\n\n\nvendas_agg = vendas.groupby('date')['sales'].sum().reset_index()\n\n# Configurar o gráfico\nplt.figure(figsize=(10, 6))\nplt.plot(vendas_agg['date'], vendas_agg['sales'], '-')\nplt.xlabel('Data')\nplt.ylabel('Vendas')\nplt.title('Vendas por Data')\nplt.grid(True)\n\n# Mostrar o gráfico\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Aplicar os testes ADF\nnone = adfuller(vendas_treino['sales'], regression= 'n')\ndrift = adfuller(vendas_treino['sales'], regression='c')\ntrend = adfuller(vendas_treino['sales'], regression='ct')\n\n# Extrair as estatísticas dos testes ADF\n\n# Teste NONE:\nadf_none = none[0]\np_valor_none = none[1]\nvc_none = none[4]\n\n# Teste Drift:\nadf_drift = drift[0]\np_valor_drift = drift[1]\nvc_drift = drift[4]\n\n# Teste Trend:\nadf_trend = trend[0]\np_valor_trend = trend[1]\nvc_trend = trend[4]\n\n# Mostrar os resultado:\n\n# Teste NONE:\nprint('Estatística ADF (None):', adf_none)\nprint('p-valor: %f' % none[1])\nprint('Valores Críticos:')\nfor key, value in vc_none.items():\n    print('%s: %.3f' % (key, value))\n\nif adf_none &lt; none[4]['1%']:\n    print('A série indica estacionariedade em nível no teste sem intercepto e sem tendência (none).')\nelse:\n    print('Segundo o teste sem intercepto e sem tendência (none), você deve diferenciar a série.')\n\nprint('\\n')\n\n# Teste drift\nprint('Estatística ADF (Drift):', adf_drift)\nprint('p-valor: %f' % drift[1])\nprint('Valores Críticos:')\nfor key, value in vc_drift.items():\n    print('%s: %.3f' % (key, value))\n\nif adf_drift &lt; drift[4]['1%']:\n    print('A série indica estacionariedade em nível no teste com intercepto (drift).')\nelse:\n    print('Segundo o teste com intercepto (drift), você deve diferenciar a série.')\n\nprint('\\n')\n\n# Teste trend\nprint('Estatística ADF (Trend):', adf_trend)\nprint('p-valor: %f' % trend[1])\nprint('Valores Críticos:')\nfor key, value in vc_trend.items():\n    print('%s: %.3f' % (key, value))\n\nif adf_trend &lt; trend[4]['1%']:\n    print('A série indica estacionariedade em nível no teste com intercepto e tendência (trend).')\nelse:\n    print('Segundo o teste com intercepto e tendência (trend), você deve diferenciar a série.')\n\nprint('\\n')\n\n\nEstatística ADF (None): -0.48432407823739815\n\n\np-valor: 0.502556\n\n\nValores Críticos:\n\n\n1%: -2.567\n5%: -1.941\n10%: -1.617\n\n\nSegundo o teste sem intercepto e sem tendência (none), você deve diferenciar a série.\n\n\nEstatística ADF (Drift): -2.827867645777691\n\n\np-valor: 0.054402\n\n\nValores Críticos:\n\n\n1%: -3.434\n5%: -2.863\n10%: -2.568\n\n\nSegundo o teste com intercepto (drift), você deve diferenciar a série.\n\n\nEstatística ADF (Trend): -5.349081542796824\n\n\np-valor: 0.000045\n\n\nValores Críticos:\n\n\n1%: -3.964\n5%: -3.413\n10%: -3.129\n\n\nA série indica estacionariedade em nível no teste com intercepto e tendência (trend).\n\n\n\n\n\n\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n\n\ninicio = '2013-01-01'\nfim = vendas['date'].max() - timedelta(days=n)\np = 3\nd = 1\nq = 4\n\n# Criar uma cópia do DataFrame 'vendas' com as colunas 'date' e 'sales' para os dados de teste\ndados_treino = vendas_treino[['date', 'sales']].copy()\n\n# Definir a coluna 'date' como índice do DataFrame dos dados de teste\ndados_treino.set_index('date', inplace=True)\n\n# Crie uma instância do modelo ARIMA\nmodel = ARIMA(dados_treino['sales'], order=(p, d, q))\n\n# Ajuste o modelo aos dados de treinamento\nmodel_fit = model.fit()\n\n# Faça previsões usando o modelo ajustado para os próximos 15 passos\npredictions = model_fit.forecast(steps=n)\npredictions_treino = model_fit.predict(start=inicio, end=fim)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom pmdarima.arima import auto_arima\n\n# Criar o modelo Auto ARIMA\nmodel_auto = auto_arima(dados_treino['sales'], trace=True)\n\n# Ajustar o modelo aos dados de treino\n\nPerforming stepwise search to minimize aic\n ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=30593.013, Time=0.74 sec\n ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=31343.115, Time=0.02 sec\n ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=31343.179, Time=0.03 sec\n ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=31107.862, Time=0.43 sec\n ARIMA(0,1,0)(0,0,0)[0]             : AIC=31341.131, Time=0.02 sec\n ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=30612.116, Time=0.91 sec\n ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=30568.558, Time=1.01 sec\n ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=30838.227, Time=0.74 sec\n ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=31015.456, Time=0.11 sec\n ARIMA(3,1,1)(0,0,0)[0] intercept   : AIC=30571.389, Time=1.19 sec\n ARIMA(3,1,0)(0,0,0)[0] intercept   : AIC=31004.819, Time=0.14 sec\n ARIMA(3,1,2)(0,0,0)[0] intercept   : AIC=30636.005, Time=1.32 sec\n ARIMA(2,1,1)(0,0,0)[0]             : AIC=30567.039, Time=0.56 sec\n ARIMA(1,1,1)(0,0,0)[0]             : AIC=30837.309, Time=0.38 sec\n ARIMA(2,1,0)(0,0,0)[0]             : AIC=31013.482, Time=0.08 sec\n ARIMA(3,1,1)(0,0,0)[0]             : AIC=30621.892, Time=0.37 sec\n ARIMA(2,1,2)(0,0,0)[0]             : AIC=30564.302, Time=0.68 sec\n ARIMA(1,1,2)(0,0,0)[0]             : AIC=30610.986, Time=0.45 sec\n ARIMA(3,1,2)(0,0,0)[0]             : AIC=30555.521, Time=0.98 sec\n ARIMA(4,1,2)(0,0,0)[0]             : AIC=29887.763, Time=1.58 sec\n ARIMA(4,1,1)(0,0,0)[0]             : AIC=30420.878, Time=0.38 sec\n ARIMA(5,1,2)(0,0,0)[0]             : AIC=29716.577, Time=1.40 sec\n ARIMA(5,1,1)(0,0,0)[0]             : AIC=29944.233, Time=0.47 sec\n ARIMA(5,1,3)(0,0,0)[0]             : AIC=29716.541, Time=1.86 sec\n ARIMA(4,1,3)(0,0,0)[0]             : AIC=29820.992, Time=1.80 sec\n ARIMA(5,1,4)(0,0,0)[0]             : AIC=29597.841, Time=1.63 sec\n ARIMA(4,1,4)(0,0,0)[0]             : AIC=29793.836, Time=2.22 sec\n ARIMA(5,1,5)(0,0,0)[0]             : AIC=29465.590, Time=2.85 sec\n ARIMA(4,1,5)(0,0,0)[0]             : AIC=29564.776, Time=2.64 sec\n ARIMA(5,1,5)(0,0,0)[0] intercept   : AIC=29469.371, Time=2.91 sec\n\nBest model:  ARIMA(5,1,5)(0,0,0)[0]          \nTotal fit time: 29.901 seconds\n\nmodel_auto_fit = model_auto.fit(dados_treino['sales'])\n\n# Fazer previsões usando o modelo ajustado para os próximos 15 passos\npredictions_auto = model_auto.predict(n_periods=n)\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n\npredictions_treino_auto = model_fit.predict(start=inicio, end=fim)\n\n# Converter as previsões em um array numpy\npredictions_auto = np.array(predictions_auto)\n\n# Calculando o RMSE\nrmse_arima_auto = np.sqrt(mean_squared_error(valores_reais_teste, predictions_auto))\nrmse_arima_treino_auto = np.sqrt(mean_squared_error(valores_reais_treino, predictions_treino_auto))\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom pmdarima.arima import auto_arima\n\n# Criar o modelo Auto ARIMA\nmodel_auto = auto_arima(dados_treino['sales'], trace=True)\n\n# Ajustar o modelo aos dados de treino\nmodel_auto_fit = model_auto.fit(dados_treino['sales'])\n\n# Fazer previsões usando o modelo ajustado para os próximos 15 passos\npredictions_auto = model_auto.predict(n_periods=n)\npredictions_treino_auto = model_fit.predict(start=inicio, end=fim)\n\n# Converter as previsões em um array numpy\npredictions_auto = np.array(predictions_auto)\n\n# Calculando o RMSE\nrmse_arima_auto = np.sqrt(mean_squared_error(valores_reais_teste, predictions_auto))\nrmse_arima_treino_auto = np.sqrt(mean_squared_error(valores_reais_treino, predictions_treino_auto))\n\n\n\n\n\n\n\n\n\n\nepocas = 200\n\nbase_nivel = pd.DataFrame({\n    'V1': vendas['sales'].shift(0),\n    'V2': vendas['sales'].shift(1),\n    'V3': vendas['sales'].shift(2),\n    'V4': vendas['oil_price'].shift(0),\n    'V5': vendas['Ano'].shift(0),\n    'V6': vendas['Mês'].shift(0),\n    'V7': vendas['Dia'].shift(0),\n})\n\nscaler = MinMaxScaler()\nbase = pd.DataFrame(scaler.fit_transform(base_nivel), columns=base_nivel.columns)\n\nbase = base.iloc[3:]\n\ny_treino = base['V1'].astype(float)\nx_treino = base.drop('V1', axis=1)\n\ny_teste = base.tail(n)['V1'].astype(float)\nx_teste = base.tail(n).drop('V1', axis=1)\n\nfrom keras import backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n\n# Criando a arquitetura da rede neural:\nmodelo_mlp = Sequential()\nmodelo_mlp.add(Dense(units=50, activation='relu', input_dim=x_treino.shape[1]))\nmodelo_mlp.add(Dense(units=1, activation='linear'))\n\n# Treinando a rede neural:\nmodelo_mlp.compile(loss=root_mean_squared_error, optimizer='adam', metrics=['mae'])\nresultado_mlp = modelo_mlp.fit(x_treino, y_treino, epochs=epocas, batch_size=32, validation_data=(x_teste, y_teste))\n\n# Valores mínimos e máximos para desnormalizar\ny_min = base_nivel['V1'].min()\ny_max = base_nivel['V1'].max()\n\n# Extrair valores de previsão do modelo\nprevisao_treino = modelo_mlp.predict(x_treino)\nprevisao_teste = modelo_mlp.predict(x_teste)\n\n# Desnormalização da previsão\nprevisao_desnormalizada = previsao_teste * (y_max - y_min) + y_min\nprevisao_treino_desnormalizada = previsao_treino * (y_max - y_min) + y_min\ny_teste_desnormalizado = y_teste * (y_max - y_min) + y_min\ny_treino_desnormalizado = y_treino * (y_max - y_min) + y_min\n\n\n\n\n\n\n\n\n\n\n\n(array([17364., 17368., 17372., 17376., 17379., 17383., 17387., 17391.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\n\n\n\n\n\n\n\n\n\n\n\n# Criando a arquitetura da rede neural LSTM:\nmodelo_lstm = Sequential()\nmodelo_lstm.add(LSTM(units=50, activation='relu', input_shape=(x_treino.shape[1], 1)))\nmodelo_lstm.add(Dense(units=10, activation='relu'))\nmodelo_lstm.add(Dense(units=1, activation='linear'))\n\n# Reshape dos dados de entrada para o formato [samples, timesteps, features]\nx_treino_lstm = x_treino.values.reshape((x_treino.shape[0], x_treino.shape[1], 1))\nx_teste_lstm = x_teste.values.reshape((x_teste.shape[0], x_teste.shape[1], 1))\n\n# Treinando a rede neural LSTM:\nmodelo_lstm.compile(loss=root_mean_squared_error, optimizer='adam', metrics=['mae'])\nresultado_lstm = modelo_lstm.fit(x_treino_lstm, y_treino, epochs=epocas, batch_size=15, validation_data=(x_teste_lstm, y_teste))\n\n# Extrair valores de previsão do modelo LSTM\nprevisao_treino_lstm = modelo_lstm.predict(x_treino_lstm)\nprevisao_teste_lstm = modelo_lstm.predict(x_teste_lstm)\n\n# Desnormalização da previsão LSTM\nprevisao_desnormalizada_lstm = previsao_teste_lstm * (y_max - y_min) + y_min\nprevisao_treino_desnormalizada_lstm = previsao_treino_lstm * (y_max - y_min) + y_min\ny_teste_desnormalizado = y_teste * (y_max - y_min) + y_min\ny_treino_desnormalizado = y_treino * (y_max - y_min) + y_min\n\n\n\n\n\n\n\n\n\n\n\n\n\n(array([17364., 17368., 17372., 17376., 17379., 17383., 17387., 17391.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\n\n\n\n\n\n¹A modelagem manual do ARIMA forneceu menor precisão que sua vertente automática, portanto foi ocultada.\n²O modelo de rede neural MLP teve maior overfitting e menor precisão na amostra de teste, portanto foi ocultada.\n\n\n\n\n\nRMSE Modelo ARIMA no treino:  2277.85\n\n\nRMSE Modelo ARIMA no teste:  2926.24\n\n\nRMSE Modelo ARIMA_AUTO no treino:  2277.85\n\n\nRMSE Modelo ARIMA_AUTO no teste:  2234.24\n\n\nRMSE Modelo MLP no treino:    8454.04\n\n\nRMSE Modelo MLP no teste:    10354.03\n\n\nRMSE Modelo LSTM no treino:    8515.02\n\n\nRMSE Modelo LSTM no teste:    10049.03\n\n\n\n\n Retorne a página principal"
  },
  {
    "objectID": "favorita_sales.html#importações",
    "href": "favorita_sales.html#importações",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "Primeiramente, vamos realizar as importações das bibliotecas que serão utilizadas:\n\n# Importação das bibliotecas\n\nimport keras\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.graph_objects as go\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom statsmodels.tsa.stattools import adfuller\nfrom datetime import timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pmdarima.arima import auto_arima\nfrom pmdarima import arima\n\n\n\n\n# Importação dos datasets\n\noil = pd.read_csv('oil.csv')\nvendas = pd.read_csv('train.csv')\nlojas = pd.read_csv('stores.csv')\ntransacoes = pd.read_csv('transactions.csv')\nferiados = pd.read_csv('holidays_events.csv')"
  },
  {
    "objectID": "favorita_sales.html#transformações-nos-dados",
    "href": "favorita_sales.html#transformações-nos-dados",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "# Definição das características da previsão\n\nnumero_loja = 44 # Número da loja que será prevista\nn = 30 # Número de dias que será previsto no modelo\nfamilia = 'BEVERAGES' # Família de produtos que será prevista\n\n\n# Transformações nos dados\n\n# Unir os datasets\nvendas = pd.merge(vendas, oil, how='left', on='date')\nvendas = pd.merge(vendas, lojas, how='left', on='store_nbr')\n\n# Filtrar apenas a loja que será projetada\nvendas = vendas[vendas['store_nbr'] == numero_loja]\nvendas = vendas[vendas['family'] == familia]\nvendas = vendas.reset_index(drop=True)\n\n# Transformações nas informações do preço do petróleo\nvendas = vendas.rename(columns={\"dcoilwtico\":\"oil_price\"}) # Renomear a coluna para oil_price\nvendas['oil_price'] = vendas['oil_price'].fillna(vendas['oil_price'].mode()[0]) # Valores NA preenchidos com a moda\n\ndel feriados, oil, lojas # Deletar bases não utilizadas após uniões\n\n# Converter  as datas para formato correto\nvendas['date'] = pd.to_datetime(vendas['date']) # Converter a coluna 'date' para o tipo datetime\nvendas['cluster'] = vendas['cluster'].astype('category')\n#dummies = pd.get_dummies(vendas['family'])\n#vendas = pd.concat([vendas, dummies], axis=1)\n\n# Removendo colunas que não serão utilizadas\nvendas = vendas.drop('id', axis=1)\nvendas = vendas.drop('onpromotion', axis=1)\nvendas = vendas.drop('city', axis=1)\nvendas = vendas.drop('state', axis=1)\nvendas = vendas.drop('store_nbr', axis=1)\nvendas = vendas.drop('type', axis=1)\n\n# Visualização do dataframe vendas para verificação\n#display(vendas)\nvendas.info() #Verificar os tipos das variaveis na base\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1684 entries, 0 to 1683\nData columns (total 5 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       1684 non-null   datetime64[ns]\n 1   family     1684 non-null   object        \n 2   sales      1684 non-null   float64       \n 3   oil_price  1684 non-null   float64       \n 4   cluster    1684 non-null   category      \ndtypes: category(1), datetime64[ns](1), float64(2), object(1)\nmemory usage: 54.5+ KB"
  },
  {
    "objectID": "favorita_sales.html#divisão-entre-amostra-de-treino-e-teste",
    "href": "favorita_sales.html#divisão-entre-amostra-de-treino-e-teste",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "# Divisão entre amostra de treino e teste\ndata_corte = vendas['date'].max() - timedelta(days=n) #Calcula a data de corte entre treino e teste\nvendas_treino = vendas.loc[vendas['date'] &lt;= data_corte]\nvendas_teste = vendas.loc[vendas['date'] &gt; data_corte]\n\n# Modificar estrutura de data para colunas\nvendas['Ano'] = vendas['date'].dt.year\nvendas['Mês'] = vendas['date'].dt.month\nvendas['Dia'] = vendas['date'].dt.day\n#vendas = vendas.drop('date', axis=1)\n\nvendas['Ano'] = vendas['Ano'].astype('category')\nvendas['Mês'] = vendas['Mês'].astype('category')\nvendas['Dia'] = vendas['Dia'].astype('category')\n\n#display(vendas_treino)\n#display(vendas_teste)"
  },
  {
    "objectID": "favorita_sales.html#análise-gráfica",
    "href": "favorita_sales.html#análise-gráfica",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "vendas_agg = vendas.groupby('date')['sales'].sum().reset_index()\n\n# Configurar o gráfico\nplt.figure(figsize=(10, 6))\nplt.plot(vendas_agg['date'], vendas_agg['sales'], '-')\nplt.xlabel('Data')\nplt.ylabel('Vendas')\nplt.title('Vendas por Data')\nplt.grid(True)\n\n# Mostrar o gráfico\nplt.show()"
  },
  {
    "objectID": "favorita_sales.html#modelagem",
    "href": "favorita_sales.html#modelagem",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "# Aplicar os testes ADF\nnone = adfuller(vendas_treino['sales'], regression= 'n')\ndrift = adfuller(vendas_treino['sales'], regression='c')\ntrend = adfuller(vendas_treino['sales'], regression='ct')\n\n# Extrair as estatísticas dos testes ADF\n\n# Teste NONE:\nadf_none = none[0]\np_valor_none = none[1]\nvc_none = none[4]\n\n# Teste Drift:\nadf_drift = drift[0]\np_valor_drift = drift[1]\nvc_drift = drift[4]\n\n# Teste Trend:\nadf_trend = trend[0]\np_valor_trend = trend[1]\nvc_trend = trend[4]\n\n# Mostrar os resultado:\n\n# Teste NONE:\nprint('Estatística ADF (None):', adf_none)\nprint('p-valor: %f' % none[1])\nprint('Valores Críticos:')\nfor key, value in vc_none.items():\n    print('%s: %.3f' % (key, value))\n\nif adf_none &lt; none[4]['1%']:\n    print('A série indica estacionariedade em nível no teste sem intercepto e sem tendência (none).')\nelse:\n    print('Segundo o teste sem intercepto e sem tendência (none), você deve diferenciar a série.')\n\nprint('\\n')\n\n# Teste drift\nprint('Estatística ADF (Drift):', adf_drift)\nprint('p-valor: %f' % drift[1])\nprint('Valores Críticos:')\nfor key, value in vc_drift.items():\n    print('%s: %.3f' % (key, value))\n\nif adf_drift &lt; drift[4]['1%']:\n    print('A série indica estacionariedade em nível no teste com intercepto (drift).')\nelse:\n    print('Segundo o teste com intercepto (drift), você deve diferenciar a série.')\n\nprint('\\n')\n\n# Teste trend\nprint('Estatística ADF (Trend):', adf_trend)\nprint('p-valor: %f' % trend[1])\nprint('Valores Críticos:')\nfor key, value in vc_trend.items():\n    print('%s: %.3f' % (key, value))\n\nif adf_trend &lt; trend[4]['1%']:\n    print('A série indica estacionariedade em nível no teste com intercepto e tendência (trend).')\nelse:\n    print('Segundo o teste com intercepto e tendência (trend), você deve diferenciar a série.')\n\nprint('\\n')\n\n\nEstatística ADF (None): -0.48432407823739815\n\n\np-valor: 0.502556\n\n\nValores Críticos:\n\n\n1%: -2.567\n5%: -1.941\n10%: -1.617\n\n\nSegundo o teste sem intercepto e sem tendência (none), você deve diferenciar a série.\n\n\nEstatística ADF (Drift): -2.827867645777691\n\n\np-valor: 0.054402\n\n\nValores Críticos:\n\n\n1%: -3.434\n5%: -2.863\n10%: -2.568\n\n\nSegundo o teste com intercepto (drift), você deve diferenciar a série.\n\n\nEstatística ADF (Trend): -5.349081542796824\n\n\np-valor: 0.000045\n\n\nValores Críticos:\n\n\n1%: -3.964\n5%: -3.413\n10%: -3.129\n\n\nA série indica estacionariedade em nível no teste com intercepto e tendência (trend).\n\n\n\n\n\n\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n\n\ninicio = '2013-01-01'\nfim = vendas['date'].max() - timedelta(days=n)\np = 3\nd = 1\nq = 4\n\n# Criar uma cópia do DataFrame 'vendas' com as colunas 'date' e 'sales' para os dados de teste\ndados_treino = vendas_treino[['date', 'sales']].copy()\n\n# Definir a coluna 'date' como índice do DataFrame dos dados de teste\ndados_treino.set_index('date', inplace=True)\n\n# Crie uma instância do modelo ARIMA\nmodel = ARIMA(dados_treino['sales'], order=(p, d, q))\n\n# Ajuste o modelo aos dados de treinamento\nmodel_fit = model.fit()\n\n# Faça previsões usando o modelo ajustado para os próximos 15 passos\npredictions = model_fit.forecast(steps=n)\npredictions_treino = model_fit.predict(start=inicio, end=fim)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom pmdarima.arima import auto_arima\n\n# Criar o modelo Auto ARIMA\nmodel_auto = auto_arima(dados_treino['sales'], trace=True)\n\n# Ajustar o modelo aos dados de treino\n\nPerforming stepwise search to minimize aic\n ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=30593.013, Time=0.74 sec\n ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=31343.115, Time=0.02 sec\n ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=31343.179, Time=0.03 sec\n ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=31107.862, Time=0.43 sec\n ARIMA(0,1,0)(0,0,0)[0]             : AIC=31341.131, Time=0.02 sec\n ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=30612.116, Time=0.91 sec\n ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=30568.558, Time=1.01 sec\n ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=30838.227, Time=0.74 sec\n ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=31015.456, Time=0.11 sec\n ARIMA(3,1,1)(0,0,0)[0] intercept   : AIC=30571.389, Time=1.19 sec\n ARIMA(3,1,0)(0,0,0)[0] intercept   : AIC=31004.819, Time=0.14 sec\n ARIMA(3,1,2)(0,0,0)[0] intercept   : AIC=30636.005, Time=1.32 sec\n ARIMA(2,1,1)(0,0,0)[0]             : AIC=30567.039, Time=0.56 sec\n ARIMA(1,1,1)(0,0,0)[0]             : AIC=30837.309, Time=0.38 sec\n ARIMA(2,1,0)(0,0,0)[0]             : AIC=31013.482, Time=0.08 sec\n ARIMA(3,1,1)(0,0,0)[0]             : AIC=30621.892, Time=0.37 sec\n ARIMA(2,1,2)(0,0,0)[0]             : AIC=30564.302, Time=0.68 sec\n ARIMA(1,1,2)(0,0,0)[0]             : AIC=30610.986, Time=0.45 sec\n ARIMA(3,1,2)(0,0,0)[0]             : AIC=30555.521, Time=0.98 sec\n ARIMA(4,1,2)(0,0,0)[0]             : AIC=29887.763, Time=1.58 sec\n ARIMA(4,1,1)(0,0,0)[0]             : AIC=30420.878, Time=0.38 sec\n ARIMA(5,1,2)(0,0,0)[0]             : AIC=29716.577, Time=1.40 sec\n ARIMA(5,1,1)(0,0,0)[0]             : AIC=29944.233, Time=0.47 sec\n ARIMA(5,1,3)(0,0,0)[0]             : AIC=29716.541, Time=1.86 sec\n ARIMA(4,1,3)(0,0,0)[0]             : AIC=29820.992, Time=1.80 sec\n ARIMA(5,1,4)(0,0,0)[0]             : AIC=29597.841, Time=1.63 sec\n ARIMA(4,1,4)(0,0,0)[0]             : AIC=29793.836, Time=2.22 sec\n ARIMA(5,1,5)(0,0,0)[0]             : AIC=29465.590, Time=2.85 sec\n ARIMA(4,1,5)(0,0,0)[0]             : AIC=29564.776, Time=2.64 sec\n ARIMA(5,1,5)(0,0,0)[0] intercept   : AIC=29469.371, Time=2.91 sec\n\nBest model:  ARIMA(5,1,5)(0,0,0)[0]          \nTotal fit time: 29.901 seconds\n\nmodel_auto_fit = model_auto.fit(dados_treino['sales'])\n\n# Fazer previsões usando o modelo ajustado para os próximos 15 passos\npredictions_auto = model_auto.predict(n_periods=n)\n\nC:\\Users\\ALEXAN~1\\ANACON~1\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n\npredictions_treino_auto = model_fit.predict(start=inicio, end=fim)\n\n# Converter as previsões em um array numpy\npredictions_auto = np.array(predictions_auto)\n\n# Calculando o RMSE\nrmse_arima_auto = np.sqrt(mean_squared_error(valores_reais_teste, predictions_auto))\nrmse_arima_treino_auto = np.sqrt(mean_squared_error(valores_reais_treino, predictions_treino_auto))\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom pmdarima.arima import auto_arima\n\n# Criar o modelo Auto ARIMA\nmodel_auto = auto_arima(dados_treino['sales'], trace=True)\n\n# Ajustar o modelo aos dados de treino\nmodel_auto_fit = model_auto.fit(dados_treino['sales'])\n\n# Fazer previsões usando o modelo ajustado para os próximos 15 passos\npredictions_auto = model_auto.predict(n_periods=n)\npredictions_treino_auto = model_fit.predict(start=inicio, end=fim)\n\n# Converter as previsões em um array numpy\npredictions_auto = np.array(predictions_auto)\n\n# Calculando o RMSE\nrmse_arima_auto = np.sqrt(mean_squared_error(valores_reais_teste, predictions_auto))\nrmse_arima_treino_auto = np.sqrt(mean_squared_error(valores_reais_treino, predictions_treino_auto))\n\n\n\n\n\n\n\n\n\n\nepocas = 200\n\nbase_nivel = pd.DataFrame({\n    'V1': vendas['sales'].shift(0),\n    'V2': vendas['sales'].shift(1),\n    'V3': vendas['sales'].shift(2),\n    'V4': vendas['oil_price'].shift(0),\n    'V5': vendas['Ano'].shift(0),\n    'V6': vendas['Mês'].shift(0),\n    'V7': vendas['Dia'].shift(0),\n})\n\nscaler = MinMaxScaler()\nbase = pd.DataFrame(scaler.fit_transform(base_nivel), columns=base_nivel.columns)\n\nbase = base.iloc[3:]\n\ny_treino = base['V1'].astype(float)\nx_treino = base.drop('V1', axis=1)\n\ny_teste = base.tail(n)['V1'].astype(float)\nx_teste = base.tail(n).drop('V1', axis=1)\n\nfrom keras import backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n\n# Criando a arquitetura da rede neural:\nmodelo_mlp = Sequential()\nmodelo_mlp.add(Dense(units=50, activation='relu', input_dim=x_treino.shape[1]))\nmodelo_mlp.add(Dense(units=1, activation='linear'))\n\n# Treinando a rede neural:\nmodelo_mlp.compile(loss=root_mean_squared_error, optimizer='adam', metrics=['mae'])\nresultado_mlp = modelo_mlp.fit(x_treino, y_treino, epochs=epocas, batch_size=32, validation_data=(x_teste, y_teste))\n\n# Valores mínimos e máximos para desnormalizar\ny_min = base_nivel['V1'].min()\ny_max = base_nivel['V1'].max()\n\n# Extrair valores de previsão do modelo\nprevisao_treino = modelo_mlp.predict(x_treino)\nprevisao_teste = modelo_mlp.predict(x_teste)\n\n# Desnormalização da previsão\nprevisao_desnormalizada = previsao_teste * (y_max - y_min) + y_min\nprevisao_treino_desnormalizada = previsao_treino * (y_max - y_min) + y_min\ny_teste_desnormalizado = y_teste * (y_max - y_min) + y_min\ny_treino_desnormalizado = y_treino * (y_max - y_min) + y_min\n\n\n\n\n\n\n\n\n\n\n\n(array([17364., 17368., 17372., 17376., 17379., 17383., 17387., 17391.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\n\n\n\n\n\n\n\n\n\n\n\n# Criando a arquitetura da rede neural LSTM:\nmodelo_lstm = Sequential()\nmodelo_lstm.add(LSTM(units=50, activation='relu', input_shape=(x_treino.shape[1], 1)))\nmodelo_lstm.add(Dense(units=10, activation='relu'))\nmodelo_lstm.add(Dense(units=1, activation='linear'))\n\n# Reshape dos dados de entrada para o formato [samples, timesteps, features]\nx_treino_lstm = x_treino.values.reshape((x_treino.shape[0], x_treino.shape[1], 1))\nx_teste_lstm = x_teste.values.reshape((x_teste.shape[0], x_teste.shape[1], 1))\n\n# Treinando a rede neural LSTM:\nmodelo_lstm.compile(loss=root_mean_squared_error, optimizer='adam', metrics=['mae'])\nresultado_lstm = modelo_lstm.fit(x_treino_lstm, y_treino, epochs=epocas, batch_size=15, validation_data=(x_teste_lstm, y_teste))\n\n# Extrair valores de previsão do modelo LSTM\nprevisao_treino_lstm = modelo_lstm.predict(x_treino_lstm)\nprevisao_teste_lstm = modelo_lstm.predict(x_teste_lstm)\n\n# Desnormalização da previsão LSTM\nprevisao_desnormalizada_lstm = previsao_teste_lstm * (y_max - y_min) + y_min\nprevisao_treino_desnormalizada_lstm = previsao_treino_lstm * (y_max - y_min) + y_min\ny_teste_desnormalizado = y_teste * (y_max - y_min) + y_min\ny_treino_desnormalizado = y_treino * (y_max - y_min) + y_min"
  },
  {
    "objectID": "favorita_sales.html#comparar-resultados-dos-modelos",
    "href": "favorita_sales.html#comparar-resultados-dos-modelos",
    "title": "Previsão de demanda - Corporación Favorita",
    "section": "",
    "text": "(array([17364., 17368., 17372., 17376., 17379., 17383., 17387., 17391.]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\n\n\n\n\n\n¹A modelagem manual do ARIMA forneceu menor precisão que sua vertente automática, portanto foi ocultada.\n²O modelo de rede neural MLP teve maior overfitting e menor precisão na amostra de teste, portanto foi ocultada.\n\n\n\n\n\nRMSE Modelo ARIMA no treino:  2277.85\n\n\nRMSE Modelo ARIMA no teste:  2926.24\n\n\nRMSE Modelo ARIMA_AUTO no treino:  2277.85\n\n\nRMSE Modelo ARIMA_AUTO no teste:  2234.24\n\n\nRMSE Modelo MLP no treino:    8454.04\n\n\nRMSE Modelo MLP no teste:    10354.03\n\n\nRMSE Modelo LSTM no treino:    8515.02\n\n\nRMSE Modelo LSTM no teste:    10049.03\n\n\n\n\n Retorne a página principal"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Previsão de Séries Temporais",
    "section": "",
    "text": "Primeiramente, a partir de uma perscpectiva analítica de dados, vamos classificar uma série temporal como dados indexados no tempo. Podendo ser s´ries de peços, PIB, inflação, desemprego, índices de atividade econômica, etc.\nEm econometria (ramo da estatística responsável pelo estudo de dados econômicos e financeiros) de séries temporais temos:\n\nDesenvolvimento de modelos para inferência e previsão;\nPrevisão como mecanismo de redução de incertezas;\nInstrumento de apoio na tomada de decisão.\n\nO futuro é incerto e a medida que criamos cenários prováveis para ele, temos maior assertividade na tomada de decisão. Quanto mais acurado o modelo, melhor a decisão.\n\nOs modelos econométricos são modelos de equações de diferenças. Desejamos explicar uma determinada variável a partir dos seus próprios valores passados, pelo tempo e outras variáveis que podem influenciá-la.\nNesses modelos, temos a hipótese de dependência temporal, ou seja, pressupomos que o passado influencia o futuro. E a partir desse pressuposto construímos uma representação matemática que explique essa relação, o que chamamos de modelo estatístico com componente estocástico (com variável aleatória).\n\n\nO que é um componente estocástico?\n\n\nEm termos simples é quando definimos uma variável aleatória que tenha dependência no tempo. Ainda, se essa v.a. é indexada no tempo, chamamos de processo estocástico.\n\n\nEstrutura de mapeamento de séries temporais por eq. diferenças:\n\\[Yt = f (Yt−1, Yt−2, . . . , Yt−p,t, X1,t−1, . . . , X2,t−1, . . . , ϵt, ϵt−1, . . .)\\]"
  },
  {
    "objectID": "index.html#previsão-de-séries-temporais",
    "href": "index.html#previsão-de-séries-temporais",
    "title": "Previsão de Séries Temporais",
    "section": "",
    "text": "Primeiramente, a partir de uma perscpectiva analítica de dados, vamos classificar uma série temporal como dados indexados no tempo. Podendo ser s´ries de peços, PIB, inflação, desemprego, índices de atividade econômica, etc.\nEm econometria (ramo da estatística responsável pelo estudo de dados econômicos e financeiros) de séries temporais temos:\n\nDesenvolvimento de modelos para inferência e previsão;\nPrevisão como mecanismo de redução de incertezas;\nInstrumento de apoio na tomada de decisão.\n\nO futuro é incerto e a medida que criamos cenários prováveis para ele, temos maior assertividade na tomada de decisão. Quanto mais acurado o modelo, melhor a decisão.\n\nOs modelos econométricos são modelos de equações de diferenças. Desejamos explicar uma determinada variável a partir dos seus próprios valores passados, pelo tempo e outras variáveis que podem influenciá-la.\nNesses modelos, temos a hipótese de dependência temporal, ou seja, pressupomos que o passado influencia o futuro. E a partir desse pressuposto construímos uma representação matemática que explique essa relação, o que chamamos de modelo estatístico com componente estocástico (com variável aleatória).\n\n\nO que é um componente estocástico?\n\n\nEm termos simples é quando definimos uma variável aleatória que tenha dependência no tempo. Ainda, se essa v.a. é indexada no tempo, chamamos de processo estocástico.\n\n\nEstrutura de mapeamento de séries temporais por eq. diferenças:\n\\[Yt = f (Yt−1, Yt−2, . . . , Yt−p,t, X1,t−1, . . . , X2,t−1, . . . , ϵt, ϵt−1, . . .)\\]"
  },
  {
    "objectID": "index.html#modelos-arima",
    "href": "index.html#modelos-arima",
    "title": "Previsão de Séries Temporais",
    "section": "Modelos ARIMA",
    "text": "Modelos ARIMA\nO modelo ARIMA (AutoRegressive Integrated Moving Average) é um modelo de previsão de séries temporais que descreve a relação entre uma variável dependente e suas observações passadas, bem como seus erros de previsão passados. A sigla ARIMA é composta pelas iniciais de suas três componentes: AR (AutoRegressivo), I (Integrated) e MA (Média Móvel).\n\nAutoRegressivo (AR): a componente AR do modelo ARIMA usa a relação linear entre uma variável e seus valores passados para prever seus valores futuros. O termo “auto” refere-se ao fato de que a variável é regredida contra si mesma. A ordem do modelo AR é determinada pelo número de lags passados usados na regressão. Por exemplo, um modelo AR(1) usa apenas um lag passado, enquanto um modelo AR(2) usa dois lags passados.\nIntegrado (I): a componente I do modelo ARIMA é responsável por diferenciar a série temporal para torná-la estacionária. Uma série temporal é considerada estacionária se suas propriedades estatísticas, como média e variância, permanecem constantes ao longo do tempo. A diferenciação pode ser feita uma ou mais vezes, dependendo da estacionariedade da série.\nMédia Móvel (MA): a componente MA do modelo ARIMA usa a relação linear entre uma variável e seus erros de previsão passados para prever seus valores futuros. O termo “média móvel” refere-se ao fato de que a variável é regredida contra uma média móvel dos seus erros de previsão passados. A ordem do modelo MA é determinada pelo número de erros de previsão passados usados na regressão. Por exemplo, um modelo MA(1) usa apenas um erro de previsão passado, enquanto um modelo MA(2) usa dois erros de previsão passados.\n\nAssim, o modelo ARIMA é uma combinação dessas três componentes, AR, I e MA, que são usadas em conjunto para prever valores futuros de uma série temporal. A ordem de cada componente é representada por um conjunto de três parâmetros (p, d, q), que correspondem ao número de lags da componente AR (p), o número de diferenciações da componente I (d), e o número de erros de previsão da componente MA (q), respectivamente.\nResumidamente estipular três características principais dos modelos ARIMA:\n\nTécnica estatística baseada em equações de diferenças;\n\nRobusta para séries temporais com diferentes dinâmicas, ou seja, distintos tipos de séries temporais;\n\nCondição restritiva para estabilidade: as séries temporais tem de ser estacionárias.\n\nTambém existem os modelos ARMAX, que incluem variáveis exógenas\n\n\nO que é estacionariedade?\n\n\nDe forma simplificada, séries estacionárias são séries cujos momentos estatísticos (média, variância e covariância) são constantes ao longo do tempo.\n\n\n\nNotação: \\[{Yt}, t = 1,2,...,n\\]\nSérie Yt, com média e variância finitos, é covariância estacionária se:\n\\(E(yt) = E(yt−s ) = µ → média\\)\nOu seja, o valor médio é constante ao longo tempo.\n\\(E[(yt − µ)²] = E[(yt−s − µ)²] = σ² y → variância\\)\nOu seja, a variânvia é constante ao longo do tempo.\n\\(E[(yt − µ)(yt−s − µ)] = E[(yt−j − µ)(yt−j−s − µ)] = γs → covariância\\)\nA covariância, ou autocovariância nesse caso, é constante ao longo do tempo.\nµ, σ2 y , γs → constantes e ∀ t e t − s.\nSéries com tendência, por exemplo, são não estacionárias (pois sua média muda com o tempo, seja para baixo com tendência de queda ou para cima com tendência de aumento).\n\nTestes de estacionariedade\nOs testes de estacionariedade, também chamados de teste de raíz unitária, nos trará a identificação da série quanto à estacionariedade. Caso a série não seja, teremos de realizar uma transformação.\n\nTesta-se \\(H0: a = 1\\) em \\(Yt = ayt-1 + ϵt\\) ~ \\(RB(0,σ²)\\)\nSe a raíz for igual a 1, a série possui tendência, é não estacionária e portanto, o processo é explosivo (não estável).\nUtilizamos o teste Dickey-Fuller Aumentado (ADF), clássico na literatura.\n\nExistem três especificações do teste ADF:\n\nSem intercepto e sem tendência (none);\nCom intercepto (drift);\nCom intercepto e tendência (trend).\n\nAplicaremos os três teste e veremos se há convergência de rejeição da hipótese nula no resultado.\nCaso não haja rejeição da hipótese nula, ou seja, caso a série seja não estacionária, teremos que realizar uma transformação usando diferenciação:\n\\[∆yt−1 = yt − yt−1\\]\nSe \\(∆yt\\) é estacionária após a primeira diferenciação, temos que I(1): integrada de ordem 1;\nSe após a primeira diferenciação a série ainda apresenta tendência, reaplicamos a diferenciação. Se após a segunda diferenciação a série apresentar estacionariedade, temos uma integrada de ordem 2 e assim sucessivamente, ou seja, diferenciamos sucessivamente até tornar a série estacionária.\nNos casos onde a série é estacionária sem diferenciação denomina-se I(O): estacionária em nível.\nOs modelos onde não há necessidade de integração, ou seja ARIMA (p,d,q), com d igual a zero, denomina-se ARMA.\n\n\nProcedimento Box & Jenkins:\n1. Identificação do modelo → Testes de raiz unitária (estacionariedade) e funções de autocorrelação\nOs critérios de informação Akaike (AIC) e Bayesiano (BIC) são usados para avaliar e comparar modelos estatísticos em termos de sua capacidade de explicar a variação dos dados com a menor complexidade possível. Ambos os critérios levam em consideração o ajuste do modelo aos dados e a complexidade do modelo, mas diferem em como a complexidade do modelo é medida.\n\\[AIC = −2 ln(L)/T + 2n/T\\]\n\\[BIC = −2ln(L)/T + n ln(T)/T\\]\nAmbos os critérios são usados para escolher entre diferentes modelos com base em sua capacidade de explicar a variação dos dados com a menor complexidade possível. No geral, modelos com valores menores de AIC ou BIC são considerados melhores porque explicam os dados com menor número de parâmetros, o que sugere maior parcimônia e generalização para dados futuros.\n2. Estimação → Estimamos várias estruturas (máxima verossimilhança)\nA etapa de estimação na metodologia Box-Jenkins tem como objetivo estimar os parâmetros do modelo ARIMA selecionado na etapa de identificação. Existem diferentes métodos de estimação disponíveis, sendo os mais comuns o método de máxima verossimilhança (MLE) e o método de mínimos quadrados (MQ).\nO método de máxima verossimilhança é baseado na maximização da função de verossimilhança, que é a probabilidade dos dados observados serem gerados pelo modelo ARIMA estimado. Esse método busca encontrar os valores dos parâmetros que maximizam a verossimilhança dos dados. O método de mínimos quadrados, por sua vez, busca minimizar a soma dos quadrados dos resíduos do modelo ARIMA estimado.\n3. Diagnóstico → Verificar se os resíduos são RB (ruído branco).\nO diagnóstico a partir do ruído branco é uma técnica utilizada na etapa de diagnóstico da metodologia Box-Jenkins para avaliar a adequação do modelo ARIMA aos dados observados. O ruído branco é uma série temporal ideal que é usada como referência para avaliar a aleatoriedade e a independência dos resíduos do modelo. A ideia é que se os resíduos do modelo forem semelhantes ao ruído branco, eles serão aleatórios e independentes e não apresentarão padrões sistemáticos de autocorrelação ou heterocedasticidade. Caso contrário, se os resíduos apresentarem padrões sistemáticos, isso pode indicar que o modelo não é adequado e precisa ser refinado.\n\n\n\nDeterminação do parâmetros AR e MA\nPara a determinação dos parâmetros a regra principal é estimar o melhor ajuste com o menor número de parâmetros, pois teremos um modelo mais simples com maior capacidade de generealização."
  },
  {
    "objectID": "index.html#divisão-entre-amostra-de-treino-e-teste",
    "href": "index.html#divisão-entre-amostra-de-treino-e-teste",
    "title": "Previsão de Séries Temporais",
    "section": "Divisão entre amostra de treino e teste",
    "text": "Divisão entre amostra de treino e teste\nNão existe uma divisão exata ou correta para determinar o horizonte de treinamento e teste do modelo. Em geral, é comum utilizar entre 70% e 80% dos dados para o treinamento e o restante para o teste. Entretanto, essa determinação é, em grande parte, empírica e depende de fatores como a própria aplicação, a disponibilidade dos dados na série histórica e o horizonte de previsão.\n\n\n Acesse aqui a previsão de demanda"
  },
  {
    "objectID": "inflacao.html",
    "href": "inflacao.html",
    "title": "Previsão de Inflação",
    "section": "",
    "text": "Registered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nFaremos uma aplicação empírica do modelo de previsão de séries temporais para prever a inflação, que torna-se uma variável crucial em economia e finanças pois é importante para condução de políticas fiscal e macroeconômica, além de ser relevante na precificação de ativos/derivativos.\nÉ uma variável de difícil predição, já que é afetada por diversos fatores e por isso tem muitas aleatoriedades e têm uma dinâmica complexa.\n\nAnálise inicial: Visualização e estatísticas descritivas\n\nTemos uma base de dados histórica do IPCA indexado no tempo. O horizonte temporal compreende de 01/2000 até 06/2022.\n\nplot(IPCA)\n\n\n\n\n\nTeste de raíz unitária (estacionariedade)\n\n\ntesteADF_none = summary(ur.df(IPCA,type = \"none\",lags = 20,selectlags = \"BIC\"))\ntesteADF_drift = summary(ur.df(IPCA,type = \"drift\",lags = 20,selectlags = \"BIC\"))\ntesteADF_trend = summary(ur.df(IPCA,type = \"trend\",lags = 20,selectlags = \"BIC\"))\n\nComo nosso teste indicou a rejeição da hipótese nula, não vamos necessitar realizar qualquer tipo de diferenciação para ajuste da série histórica, indicando uma série estacionária em nível.\n\nDividindo amostra em treino e teste\n\nIremos utilizar nesse caso os últimos 24 meses para teste.\n\nn &lt;- 24 # número de meses para teste\n\nIPCA_treino &lt;- ts(IPCA[1:(length(IPCA)-n)],start = c(2000,1),frequency = 12)\nIPCA_teste &lt;- ts(IPCA[(length(IPCA)-n+1):length(IPCA)],start = c(2020,7),frequency = 12)\n\n\nAvaliação do modelo\n\n4.1 Critérios informacionais\n4.1.1 BIC\n4.1.2 AIC\n4.2 Teste ACF\nacf(modelo$residuals)"
  },
  {
    "objectID": "RNA.html",
    "href": "RNA.html",
    "title": "Previsão de Séries Temporais",
    "section": "",
    "text": "EXPLICAÇÃO DO QUE SÃO REDES NEURAIS ARITIFICIAIS E SUA INSPIRAÇÃO NOS NEURÔNIOS BIOLÓGICOS.\nNEURONIOS ATIVADOS - ENVIANDO INFORMAÇÕES RECONHECIMENTO DE PADRÕES COMPLEXOS (APRENDER) NEURONIOS ARTIFICIAIS - CONEXÕES ENTRE NEURONIOS"
  },
  {
    "objectID": "RNA.html#redes-neurais-artificiais",
    "href": "RNA.html#redes-neurais-artificiais",
    "title": "Previsão de Séries Temporais",
    "section": "",
    "text": "EXPLICAÇÃO DO QUE SÃO REDES NEURAIS ARITIFICIAIS E SUA INSPIRAÇÃO NOS NEURÔNIOS BIOLÓGICOS.\nNEURONIOS ATIVADOS - ENVIANDO INFORMAÇÕES RECONHECIMENTO DE PADRÕES COMPLEXOS (APRENDER) NEURONIOS ARTIFICIAIS - CONEXÕES ENTRE NEURONIOS"
  }
]